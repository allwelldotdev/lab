Kubernetes and Cloud Native Essentials (LFS250)
Taken from the Linux Foundation. Link -> https://trainingportal.linuxfoundation.org/courses/kubernetes-and-cloud-native-essentials-lfs250

## Cloud Native Architecture
### Characteristics of Cloud Native Architecture
1. High level of automation
2. Self healing
3. Scalable
4. Cost Efficient
5. Easy to maintain
6. Secure by default

#### 1. High level of automation
To managing all the moving parts of a cloud native application, automation is required through software (like Terraform) and CI/CD pipelines which utilize version control systems like git. A reliable automated system also enables easier disaster recovery processes - think if you have to rebuild your whole system.

#### 2. Self Healing
Because applications and infrastructure fail from time to time, cloud native applications have the ability to restart services when they fail their health checks. Though, since apps are compartmentalized (as microservices), there is a chance that when one part of the app fails, only that part is down while others are up and running fine, until the failed part is restarted.

#### 3. Scalable
Scaling applications is the process of handling more load while still providing a seamless user experience for users. In cloud native applications, "scaling out" is the common scaing method. 

#### 4. Cost efficient
Just like scaling up your application for high traffic situations, scaling down your application and the usage-based pricing models of cloud providers can save costs if traffic is low.

#### 5. Easy to maintain
Using _Microservices_ allows to break down applications in smaller pieces and make them more portable, easier to test and to distribute across multiple teams.

#### 6. Secure by default
In the cloud native environment, patterns like zero trust computing ensure secure procedures by requiring authentication from every user and process.

> While these patterns and technologies provide full advantage if they run in the cloud, they can also offer a lot of benefits when applied to on-premises systems. Last but not least, they allow a smoother transition if you migrate your applications and infrastructure to the cloud.

### Serverless
Applications that are written for serverless platforms have even stricter requirements for cloud native architecture, but at the same time can benefit most from them. Writing small, stateless applications make them a perfect fit for event or data streams, scheduled tasks, business logic or batch processing.

### Open Standards
Under the umbrella of the Linux Foundation, the Open Container Initiative provides two standards which define the way how to build and run containers.

Open standards like this help and complement other systems like Kubernetes, which is the _de facto_ standard platform for orchestrating containers. A few standards that will discover in the following chapters are:

- [OCI Spec](https://opencontainers.org/): image, runtime and distribution specification on how to run, build an distribute containers
- [Container Network Interface (CNI)](https://github.com/containernetworking/cni): A specification on how to implement networking for Containers.
- [Container Runtime Interface (CRI)](https://github.com/kubernetes/cri-api): A specification on how to implement container runtimes in container orchestration systems.
- [Container Storage Interface (CSI)](https://github.com/container-storage-interface/spec): A specification on how to implement storage in container orchestration systems.
- [Service Mesh Interface (SMI)](https://smi-spec.io/): A specification on how to implement Service Meshes in container orchestration systems with a focus on Kubernetes.

Following this approach, other systems like Prometheus or OpenTelemetry evolved and thrived in this ecosystem and provide additional standards for monitoring and observability.

> Interesting resources on CNCF technologies, terminologies, and understanding nomenclature.
> Check out: https://iximiuz.com/en/posts/devops-sre-and-platform-engineering/ and https://trainingportal.linuxfoundation.org/learn/course/kubernetes-and-cloud-native-essentials-lfs250/cloud-native-architecture/cloud-native-architecture?client=andela&page=9

> Stages for CNCF software sponsorship or endorsement are *sandbox*, *incubator*, *graduation* stage. This is managed by the CNCF and the Technical Oversight Committee (TOC).

## Container Orchestration
### Container Basics
Before Docker, `chroot`, which was introduced in Version 7 Unix in 1979 was used to create container-like environments within Linux machines.

How? The `chroot` command could be used to isolate a process from the root filesystem and basically "hide" the files from the process and simulate a new root directory. The isolated environment is a so-called _chroot jail_, where the files can’t be accessed by the process, but are still present on the system.

![chroot jail system](../../assets/Pasted%20image%2020250524171247.png)

To isolate a process even more than chroot can do, current Linux kernels provide features like _namespaces_ and *cgroups*.

*Namespaces* are used to isolate various resources, for example the network. A network namespace can be used to provide a complete abstraction of network interfaces and routing tables. This allows a process to have its own IP address. The Linux Kernel 5.6 currently provides 8 namespaces:

- **pid** - process ID provides a process with its own set of process IDs.
- **net** - network allows the processes to have their own network stack, including the IP address.
- **mnt** - mount abstracts the filesystem view and manages mount points.
- **ipc** - inter-process communication provides separation of named shared memory segments.
- **user** - provides process with their own set of user IDs and group IDs.
- **uts** - Unix time sharing allows processes to have their own hostname and domain name.
- **cgroup** - a newer namespace that allows a process to have its own set of cgroup root directories.
- **time** - the newest namespace can be used to virtualize the clock of the system.

*cgroups* are used to organize processes in hierarchical groups and assign them resources like memory and CPU. When you want to limit your application container to let’s say 4GB of memory, cgroups are used under the hood to ensure these limits.

![traditional vs virtualized vs container deployments](../../assets/Pasted%20image%2020250524171608.png)

In many cases it is not a question of whether you are using containers or virtual machines, rather you are using both technologies to benefit from the efficiency that containers have but still use the security advantages that the greater isolation of virtual machines bring to the table.

### Running Containers
Best way to understand the relationship between the container image and the running container is in terms of object-oriented programming - the container image is the class while the running container is an instantiation of the class.

With Docker installed, you can run containers like so:
```bash
docker run nginx
```



