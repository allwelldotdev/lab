Kubernetes and Cloud Native Essentials (LFS250)
Taken from the Linux Foundation. Link -> https://trainingportal.linuxfoundation.org/courses/kubernetes-and-cloud-native-essentials-lfs250

## Cloud Native Architecture
### Characteristics of Cloud Native Architecture
1. High level of automation
2. Self healing
3. Scalable
4. Cost Efficient
5. Easy to maintain
6. Secure by default

#### 1. High level of automation
To managing all the moving parts of a cloud native application, automation is required through software (like Terraform) and CI/CD pipelines which utilize version control systems like git. A reliable automated system also enables easier disaster recovery processes - think if you have to rebuild your whole system.

#### 2. Self Healing
Because applications and infrastructure fail from time to time, cloud native applications have the ability to restart services when they fail their health checks. Though, since apps are compartmentalized (as microservices), there is a chance that when one part of the app fails, only that part is down while others are up and running fine, until the failed part is restarted.

#### 3. Scalable
Scaling applications is the process of handling more load while still providing a seamless user experience for users. In cloud native applications, "scaling out" is the common scaing method. 

#### 4. Cost efficient
Just like scaling up your application for high traffic situations, scaling down your application and the usage-based pricing models of cloud providers can save costs if traffic is low.

#### 5. Easy to maintain
Using _Microservices_ allows to break down applications in smaller pieces and make them more portable, easier to test and to distribute across multiple teams.

#### 6. Secure by default
In the cloud native environment, patterns like zero trust computing ensure secure procedures by requiring authentication from every user and process.

> While these patterns and technologies provide full advantage if they run in the cloud, they can also offer a lot of benefits when applied to on-premises systems. Last but not least, they allow a smoother transition if you migrate your applications and infrastructure to the cloud.

### Serverless
Applications that are written for serverless platforms have even stricter requirements for cloud native architecture, but at the same time can benefit most from them. Writing small, stateless applications make them a perfect fit for event or data streams, scheduled tasks, business logic or batch processing.

### Open Standards
Under the umbrella of the Linux Foundation, the Open Container Initiative provides two standards which define the way how to build and run containers.

Open standards like this help and complement other systems like Kubernetes, which is the _de facto_ standard platform for orchestrating containers. A few standards that will discover in the following chapters are:

- [OCI Spec](https://opencontainers.org/): image, runtime and distribution specification on how to run, build an distribute containers
- [Container Network Interface (CNI)](https://github.com/containernetworking/cni): A specification on how to implement networking for Containers.
- [Container Runtime Interface (CRI)](https://github.com/kubernetes/cri-api): A specification on how to implement container runtimes in container orchestration systems.
- [Container Storage Interface (CSI)](https://github.com/container-storage-interface/spec): A specification on how to implement storage in container orchestration systems.
- [Service Mesh Interface (SMI)](https://smi-spec.io/): A specification on how to implement Service Meshes in container orchestration systems with a focus on Kubernetes.

Following this approach, other systems like Prometheus or OpenTelemetry evolved and thrived in this ecosystem and provide additional standards for monitoring and observability.

> Interesting resources on CNCF technologies, terminologies, and understanding nomenclature.
> Check out: https://iximiuz.com/en/posts/devops-sre-and-platform-engineering/ and https://trainingportal.linuxfoundation.org/learn/course/kubernetes-and-cloud-native-essentials-lfs250/cloud-native-architecture/cloud-native-architecture?client=andela&page=9

> Stages for CNCF software sponsorship or endorsement are *sandbox*, *incubator*, *graduation* stage. This is managed by the CNCF and the Technical Oversight Committee (TOC).

## Container Orchestration
### Container Basics
Before Docker, `chroot`, which was introduced in Version 7 Unix in 1979 was used to create container-like environments within Linux machines.

How? The `chroot` command could be used to isolate a process from the root filesystem and basically "hide" the files from the process and simulate a new root directory. The isolated environment is a so-called _chroot jail_, where the files can’t be accessed by the process, but are still present on the system.

![chroot jail system](../../assets/Pasted%20image%2020250524171247.png)

To isolate a process even more than chroot can do, current Linux kernels provide features like _namespaces_ and *cgroups*.

*Namespaces* are used to isolate various resources, for example the network. A network namespace can be used to provide a complete abstraction of network interfaces and routing tables. This allows a process to have its own IP address. The Linux Kernel 5.6 currently provides 8 namespaces:

- **pid** - process ID provides a process with its own set of process IDs.
- **net** - network allows the processes to have their own network stack, including the IP address.
- **mnt** - mount abstracts the filesystem view and manages mount points.
- **ipc** - inter-process communication provides separation of named shared memory segments.
- **user** - provides process with their own set of user IDs and group IDs.
- **uts** - Unix time sharing allows processes to have their own hostname and domain name.
- **cgroup** - a newer namespace that allows a process to have its own set of cgroup root directories.
- **time** - the newest namespace can be used to virtualize the clock of the system.

*cgroups* are used to organize processes in hierarchical groups and assign them resources like memory and CPU. When you want to limit your application container to let’s say 4GB of memory, cgroups are used under the hood to ensure these limits.

![traditional vs virtualized vs container deployments](../../assets/Pasted%20image%2020250524171608.png)

In many cases it is not a question of whether you are using containers or virtual machines, rather you are using both technologies to benefit from the efficiency that containers have but still use the security advantages that the greater isolation of virtual machines bring to the table.

### Running Containers
Best way to understand the relationship between the container image and the running container is in terms of object-oriented programming - the container image is the class while the running container is an instantiation of the class.

With Docker installed, you can run containers like so:
```bash
docker run nginx
```

Podman and Docker are very similar to each other. Using Podman, is as simple as running the same commands you'd normally run for Docker. For example:
```bash
podmain run --detach --publish-all nginx:1.20
```

### Building Container Images
Container images are what makes containers portable and easy to reuse on a variety of systems. [Docker](https://www.docker.com/resources/what-container) describes a container image as following:

*“A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.”*

![Docker's image spec donated to OCI in 2015](../../assets/Pasted%20image%2020250525132348.png)

Container images are built from a file known as the `Dockerfile`.
```bash
docker build -t my-python-image -f Dockerfile
```
- `-t`: specify a name or tag for your image
- `-f`: specify the name or location of the build file (which is usually `Dockerfile`)

Afterward, built images are pushed or uploaded to a registry (usually the Docker registry known as Docker Hub). You can push and pull them from Docker Hub like so:
```bash
docker push my-registry.com/my-python-image
docker pull my-registry.com/my-python-image
```

### Security
Running containers as root users, which is often the case, is a security-risk because as 'root' the container can perform administrative kernel functionality like killing other processes or modifying the host network by creating routing rules.

> Learn more about kernel capabilities in the [Docker documentation](https://docs.docker.com/engine/security/#linux-kernel-capabilities)

A new attack surface that was introduced with containers is the use of public images. Two of the most popular container image registries are [Docker Hub](https://hub.docker.com) and [Quay](https://quay.io/). You have to ensure that images you pull from these registries do not include malicious software (best tip is to pull and/or use registered images).

Sysdig has a great [blog article on how to avoid a lot of security issues and build secure container images](https://sysdig.com/blog/dockerfile-best-practices/).

The 4C's of Cloud Native security can give a rough idea what layers need to be protected if you’re using containers. Make sure to cover every layer since it’s effectively protecting the layer within. The [Kubernetes documentation](https://kubernetes.io/docs/concepts/security/overview/) is a good starting point to understand the layers.

![4Cs of Cloud Native Security](../../assets/Pasted%20image%2020250525134729.png)

### Container Orchestrating Fundamentals
If you have to manage and deploy large amounts of containers, you quickly get to the point where you need a system that helps with the management of these containers. Problems to be solved can include:

- Providing compute resources like virtual machines where containers can run on
- Schedule containers to servers in an efficient way
- Allocate resources like CPU and memory to containers
- Manage the availability of containers and replace them if they fail
- Scale containers if load increases
- Provide networking to connect containers together
- Provision storage if containers need to persist data

Container orchestration systems provide a way to build a cluster of multiple servers and host the containers on top. Most container orchestration systems consist of two parts: a _control plane_ that is responsible for the management of the containers and _worker nodes_ that actually host the containers.

Today, the industry has chosen *Kubernetes* as the standard system to orchestrate containers.

### Networking
Network namespaces allow each container to have its own unique IP address.
To make the application accessible from outside the host system, containers have the ability to map a port from the container to a port from the host system.
Container can communicate with each other across hosts through an *overlay* network.

> Most modern implementations of container networking are based on the [Container Network Interface (CNI)](https://github.com/containernetworking/cni). CNI is a standard that can be used to write or configure network plugins and makes it very easy to swap out different plugins in various container orchestration platforms.

### Service Discovery & DNS
For a long time, server management in traditional data centers was manageable. System administrators even remembered IP addresses of important systems they worked with. In container orchestration platforms things are a lot more complicated:

- Hundreds or thousands of containers with individual IP addresses
- Containers are deployed on varieties of different hosts, different data centers or even geolocations
- Containers or Services need DNS to communicate. Using IP addresses is nearly impossible
- Information about Containers must be removed from the system when they get deleted.

The solution to the problem again is automation. Instead of having a manually maintained list of servers (or in this case containers), all the information is put in a _Service Registry_. Finding other services in the network and requesting information about them is called _Service Discovery_.

#### Approaches to Service Discovery
- DNS
	- Modern DNS servers that have a service API can be used to register new services as they are created. This approach is pretty straightforward, as most organizations already have DNS servers with the appropriate capabilities.
- Key-Value-Store
	- Using a strongly consistent datastore especially to store information about services. A lot of systems are able to operate highly available with strong failover mechanisms. Popular choices, especially for clustering, are [etcd](https://github.com/coreos/etcd/), [Consul](https://www.consul.io/) or [Apache Zookeeper](https://zookeeper.apache.org/).

### Service Mesh
Because networking is such a crucial part of microservices and containers, the networking can get very complex and opaque for developers and administrators. In addition to that, a lot of functionality like monitoring, access control or encryption of the networking traffic is desired when containers communicate with each other.

Instead of implementing all of this functionality into your application, you can just start a second container that has this functionality implemented. The software you can use to manage network traffic is called a *proxy.* This is a server application that sits between a client and server and can modify or filter network traffic before it reaches the server. Popular representatives are [nginx](https://www.nginx.com/), [haproxy](http://www.haproxy.org/) or [envoy](https://www.envoyproxy.io/).

Taking this idea a step further, a *service mesh* adds a proxy server to _every_ container that you have in your architecture.

![istio service mesh](../../assets/Pasted%20image%2020250525143411.png)
**Istio Architecture**, retrieved from [istio.io](https://istio.io/v1.10/docs/ops/deployment/architecture/)

If two or more applications should encrypt their traffic when they talk to each other, it would require adding libraries and configuration and management of digital certificates that prove the identity of the involved applications.

When a service mesh is used, applications don’t talk to each other directly, but the traffic is routed through the proxies instead. The most popular service meshes at the moment are [istio](https://istio.io/) and [linkerd](https://linkerd.io/).

The proxies in a service mesh form the _data plane_. This is where networking rules are implemented and shape the traffic flow.

These rules are managed centrally in the _control plane_ of the service mesh. This is where you define how traffic flows from service A to service B and what configuration should be applied to the proxies.

So instead of writing code and installing libraries, you just write a config file where you tell the service mesh that service A and service B should always communicate encrypted.

The config is then uploaded to the control plane and distributed to the data plane to enforce the new rule.

For a long time the term "service mesh" only described a basic idea of how traffic in container platforms could be handled with proxies. The [Service Mesh Interface (SMI)](https://smi-spec.io/) project aims at defining a specification on how a service mesh from various providers can be implemented.

With a strong focus on Kubernetes, their goal is to standardize the end user experience for service meshes, as well as a standard for the providers that want to integrate with Kubernetes. You can find the current [specification](https://github.com/servicemeshinterface/smi-spec) on GitHub.

### Storage
From a storage perspective, containers have a significant flaw: they are ephemeral. To understand what the means exactly we need to understand what happens when a container gets started from a container image.

Generally speaking, container images are read-only and consist of different layers that include everything that you added during the build phase. That ensures that every time you start a container from an image you get the same behavior and functionality. As you probably can imagine, a lot of applications need to write files. To allow writing files, a read-write layer is put on top of the container image when you start a container from an image.

![container images and R/W layer](../../assets/Pasted%20image%2020250525145555.png)
**Container Layers**, retrieved from the [Docker documentation](https://docs.docker.com/storage/storagedriver/)

The problem here is that this read-write layer is lost when the container is stopped or deleted. Just like the memory of your computer gets erased when you shut it down. To persist data, you need to write it to your disk. 

If a container needs to persist data on a host, a _volume_ can be used to achieve that. The concept and technology for that is quite simple: instead of isolating the whole filesystem of a process, directories that reside on the host are passed through into the container filesystem.

![data sharing](../../assets/Pasted%20image%2020250525145710.png)
**Data is shared between two containers on the same host.**

When you orchestrate a lot of containers, persisting the data on the host where the container was started might not be the only challenge. Often, data needs to be accessed by multiple containers that are started on different host systems or when a container gets started on a different host it still should have access to its volume.

Container orchestration systems like Kubernetes can help to mitigate these problems, but always require a robust storage system that is attached to the host servers.

![k8s container storage orchestration](../../assets/Pasted%20image%2020250525145753.png)
**Storage is provisioned via a central storage system. Containers on Server A and Server B can share a volume to read and write data**

In order to keep up with the unbroken growth of various storage implementations, again, the solution was to implement a standard. The [Container Storage Interface (CSI)](https://github.com/container-storage-interface/spec) came up to offer a uniform interface which allows attaching different storage systems no matter if it’s cloud or on-premises storage.

### Additional Resources
For more resources, use:

- The History of Containers
	- [A Brief History of Containers: From the 1970s Till Now](https://blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016), by Rani Osnat (2020)
	- [It's Here: Docker 1.0](https://web.archive.org/web/20160426102954/https://blog.docker.com/2014/06/its-here-docker-1-0/), by Julien Barbier (2014)
- Chroot
	- [chroot](https://wiki.ubuntuusers.de/chroot/)
- Container Performance
	- [Container Performance Analysis at DockerCon 2017](https://www.brendangregg.com/blog/2017-05-15/container-performance-analysis-dockercon-2017.html), by Brendan Gregg
- Best Practices on How to Build Container Images
	- [Top 20 Dockerfile Best Practices](https://sysdig.com/blog/dockerfile-best-practices/), by Álvaro Iradier (2021)
	- [3 simple tricks for smaller Docker images](https://learnk8s.io/blog/smaller-docker-images), by Daniele Polencic (2019)
	- [Best practices for building containers](https://cloud.google.com/architecture/best-practices-for-building-containers)
- Alternatives to Classic Dockerfile Container Building
	- [Buildpacks vs Jib vs Dockerfile: Comparing containerization methods](https://trainingportal.linuxfoundation.org/learn/course/kubernetes-and-cloud-native-essentials-lfs250/container-orchestration/%C3%81l), by James Ward (2020)
- Service Discovery
	- [Service Discovery in a Microservices Architecture](https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/), by Chris Richardson (2015)
- Container Networking
	- [Kubernetes Networking Part 1: Networking Essentials](https://www.inovex.de/de/blog/kubernetes-networking-part-1-en/), By Simon Kurth (2021)
	- [Life of a Packet (I)](https://www.youtube.com/watch?v=0Omvgd7Hg1I), by Michael Rubin (2017)
	- [Computer Networking Introduction - Ethernet and IP (Heavily Illustrated)](https://iximiuz.com/en/posts/computer-networking-101/), by Ivan Velichko (2021)
- Container Storage
	- [Managing Persistence for Docker Containers](https://thenewstack.io/methods-dealing-container-storage/), by Janakiram MSV (2016)
- Container and Kubernetes Security
	- [Secure containerized environments with updated thread matrix for Kubernetes](https://www.microsoft.com/security/blog/2021/03/23/secure-containerized-environments-with-updated-threat-matrix-for-kubernetes/), by Yossi Weizman (2021)
- Docker Container Playground
	- [Play with Docker](https://labs.play-with-docker.com/)

## Kubernetes Fundamentals
To learn even more Kubernetes basics, you can take the Linux Foundation's free [Introduction to Kubernetes (LFS158x)](https://training.linuxfoundation.org/training/introduction-to-kubernetes/) course on edX.

Originally designed and developed by Google, Kubernetes got open-sourced in 2014, and along the release v1.0 Kubernetes was donated to the newly formed Cloud Native Computing Foundation as the very first project. A lot of cloud native technologies evolve around Kubernetes, be it low-level tools like container runtimes, monitoring or application delivery tools.

Professionals with Kubernetes skills, be it administration or developing, are highly sought after since Kubernetes has become the centerpiece of a lot of modern cloud platforms.

### Kubernetes Architecture
Kubernetes is often used as a cluster, meaning that it is spanned across multiple servers that work on different tasks and to distribute the load of a system. This is an initial design decision based on the requirements at Google where billions of containers get started every week. Given the high horizontal scalability of Kubernetes, it is possible to have clusters with thousands of server nodes, across multiple datacenters and regions.

From a high-level perspective, Kubernetes clusters consist of two different server node types that make up a cluster:

- **Control plane node(s)**  
    These are the brains of the operation. Control plane nodes contain various components which manage the cluster and control various tasks like deployment, scheduling and self-healing of containerized workloads.
- **Worker nodes**  
    The worker nodes are where applications run in your cluster. This is the only job of worker nodes and they don’t have any further logic implemented. Their behavior, like if they should start a container, is completely controlled by the control plane node.

![kubernetes architecture](../../assets/Pasted%20image%2020250525151609.png)

- Control plane nodes typically host the following services...
	- `kube-apiserver`
		- This is the centerpiece of Kubernetes. All other components interact with the api-server and this is where users would access the cluster.
	- `etcd`
		- A database which holds the state of the cluster. [etcd](https://etcd.io/) is a standalone project and not an official part of Kubernetes.
	- `kube-scheduler`
		- When a new workload should be scheduled, the kube-scheduler chooses a worker node that could fit, based on different properties like CPU and memory.
	- `kube-controller-manager`
		- Contains different non-terminating control loops that manage the state of the cluster. For example, one of these control loops can make sure that a desired number of your application is available all the time.
	- `cloud-controller-manager (optional)`
		- Can be used to interact with the API of cloud providers, to create external resources like load balancers, storage or security groups.
- Components of worker nodes...
	- `container runtime`
		- The container runtime that is responsible for running the containers on the worker node. For a long time, Docker was the most popular choice, but is now replaced in favor of other runtimes like [containerd](https://containerd.io/).
	- `kubelet`
		- A small agent that runs on every worker node in the cluster. The kubelet talks to the api-server and the container runtime to handle the final stage of starting containers.
	- `kube-proxy`
		- A network proxy that handles inside and outside communication of your cluster. Instead of managing traffic flow on its own, the kube-proxy tries to rely on the networking capabilities of the underlying operating system if possible.

> It is important to note that this design makes it possible that applications that are already started on a worker node will continue to run even if the control plane is not available. Although a lot of important functionality like scaling, scheduling new applications, etc., will not be possible while the control plane is offline.

> Kubernetes also has a concept of _namespaces_, which are not to be confused with kernel namespaces that are used to isolate containers. A Kubernetes namespace can be used to divide a cluster into _multiple virtual clusters_, which can be used for multi-tenancy when multiple teams share a cluster. Please note that Kubernetes namespaces are not suitable for strong isolation and should more be viewed like a directory on a computer where you can organize objects and manage which user has access to which folder.

### Kubernetes Setup

Setting up a Kubernetes cluster can be achieved with a lot of different methods. Creating a test "cluster" can be very easy with the right tools:

- [Minikube](https://minikube.sigs.k8s.io/docs/)
- [kind](https://kind.sigs.k8s.io/)
- [MicroK8s](https://microk8s.io/)

If you want to set up a production-grade cluster on your own hardware or virtual machines, you can choose one of the various installers:

- [kubeadm](https://kubernetes.io/docs/reference/setup-tools/kubeadm/)
- [kops](https://github.com/kubernetes/kops)
- [kubespray](https://github.com/kubernetes-sigs/kubespray)

A few vendors started packaging Kubernetes into a distribution and even offer commercial support:

- [Rancher](https://rancher.com/)
- [k3s](https://k3s.io/)
- [OpenShift](https://www.redhat.com/en/technologies/cloud-computing/openshift)
- [VMWare Tanzu](https://tanzu.vmware.com/tanzu)

The distributions often choose an opinionated approach and offer additional tools while using Kubernetes as the central piece of their framework.

If you don’t want to install and manage it yourself, you can consume it from a cloud provider:

- [Amazon (EKS)](https://aws.amazon.com/eks/)
- [Google (GKE)](https://cloud.google.com/kubernetes-engine)
- [Microsoft (AKS)](https://azure.microsoft.com/en-us/services/kubernetes-service)
- [DigitalOcean (DOKS)](https://www.digitalocean.com/products/kubernetes/)

**Interactive Tutorial - Create a Cluster**
You can learn how to set up your own Kubernetes cluster with Minikube in this [interactive tutorial](https://kubernetes.io/docs/tutorials/kubernetes-basics/create-cluster/cluster-intro/).

### Kubernetes API
The Kubernetes API is the most important component of a Kubernetes cluster. Without it, communication with the cluster is not possible, every user and every component of the cluster itself needs the api-server.

![kubernetes api](../../assets/Pasted%20image%2020250525170738.png)
**Access Control Overview**, retrieved from the [Kubernetes documentation](https://kubernetes.io/docs/concepts/security/controlling-access/).

Before a request is processed by Kubernetes, it has to go through three stages:

- **Authentication**  
    The requester needs to present a means of identity to authenticate against the API. Commonly done with a digital signed certificate ([X.509](https://en.wikipedia.org/wiki/X.509)) or with an external identity management system. Kubernetes users are _always_ externally managed. [Service Accounts](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/) can be used to authenticate technical users.
- **Authorization**  
    It is decided what the requester is allowed to do. In Kubernetes this can be done with [Role Based Access Control (RBAC)](https://kubernetes.io/docs/reference/access-authn-authz/rbac/).
- **Admission Control**  
    In the last step, [admission controllers](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/) can be used to modify or validate the request. For example, if a user tries to use a container image from an untrustworthy registry, an admission controller could block this request. Tools like the [Open Policy Agent](https://www.openpolicyagent.org/) can be used to manage admission control externally.

Like many other APIs, the Kubernetes API is implemented as a RESTful interface that is exposed over HTTPS. Through the API, a user or service can create, modify, delete or retrieve resources that reside in Kubernetes.

### Running Containers on Kubernetes
How does running a container on your local machine differ from running containers in Kubernetes? In Kubernetes, instead of starting containers directly, you define Pods as the smallest compute unit and Kubernetes translates that into a running container. We will learn more about Pods later, for now imagine it as a wrapper around a container.

When you create a Pod object in Kubernetes, several components are involved in that process, until you get containers running a node.

Here is an example using containerd:

![pods created using containerd container runtime](../../assets/Pasted%20image%2020250526101103.png)
**Running Containers in Kuberenetes**

In an effort to allow using other container runtimes than Docker, Kubernetes introduced the [Container Runtime Interface (CRI)](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) in 2016.

**Container Runtimes**:
- **containerd**
	- [containerd](https://containerd.io/) is a lightweight and performant implementation to run containers. Arguably the most popular container runtime right now. It is used by all major cloud providers for the Kubernetes As A Service products.
- **CRI-O**
	- [CRI-O](https://cri-o.io/) was created by Red Hat and with a similar code base closely related to podman and buildah.
- **Docker**
	- The standard for a long time, but never really made for container orchestration. The usage of Docker as the runtime for Kubernetes has been deprecated and removed in Kubernetes 1.24. Kubernetes has a great [blog article](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/) that answers all the questions on the matter.

The idea of containerd and CRI-O was very simple: provide a runtime that only contains the absolutely essentials to run containers. Nevertheless, they have additional features, like the ability to integrate with container runtime sandboxing tools. These tools try to solve the security problem that comes with sharing the kernel between multiple containers. The most common tools at the moment are:

- [gvisor](https://github.com/google/gvisor)  
    Made by Google, provides an application kernel that sits between the containerized process and the host kernel.
- [Kata Containers](https://katacontainers.io/)  
    A secure runtime that provides a lightweight virtual machine, but behaves like a container.

### Networking
Kubernetes networking can be very complicated and hard to understand. A lot of these concepts are not Kubernetes-related and were covered in the _Container Orchestration_ chapter. Again, we have to deal with the problem that a lot of containers need to communicate across a lot of nodes. Kubernetes distinguishes between four different networking problems that need to be solved:

1. **Container-to-Container communications**  
    This can be solved by the Pod concept as we'll learn later.
2. **Pod-to-Pod communications**  
    This can be solved with an overlay network.
3. **Pod-to-Service communications**  
    It is implemented by the kube-proxy and packet filter on the node.
4. **External-to-Service communications**  
    It is implemented by the kube-proxy and packet filter on the node.

There are different ways to implement networking in Kubernetes, but also three important requirements:

- All pods can communicate with each other across nodes.
- All nodes can communicate with all pods.
- No Network Address Translation (NAT).

To implement networking, you can choose from a variety of network vendors like:

- [Project Calico](https://www.tigera.io/project-calico/)
- [Weave](https://www.weave.works/oss/net/)
- [Cilium](https://cilium.io/)

In Kubernetes, every Pod gets its own IP address, so there is no manual configuration involved. Moreover, most Kubernetes setups include a DNS server add-on called [core-dns](https://kubernetes.io/docs/tasks/administer-cluster/coredns/), which can provide service discovery and name resolution inside the cluster.

By design, every pod can communicate with other pods on the Kubernetes cluster, however if you want to control the traffic flow at the IP address or port level, then you have to use Network Policies. Network Policies act as cluster internal firewalls. Network Policies can be defined for a set of pods or namespace with the help of a selector to specify what traffic is allowed to and from the pods that match the selector. IP-based Network Policies are defined with IP blocks (CIDR ranges). Network Policies are implemented by the _network plugin_. To use Network Policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.













